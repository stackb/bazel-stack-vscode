// Original file: proto/remote_execution.proto

import type * as grpc from '@grpc/grpc-js'
import type { MethodDefinition } from '@grpc/proto-loader'
import type { BatchReadBlobsRequest as _build_bazel_remote_execution_v2_BatchReadBlobsRequest, BatchReadBlobsRequest__Output as _build_bazel_remote_execution_v2_BatchReadBlobsRequest__Output } from '../../../../../build/bazel/remote/execution/v2/BatchReadBlobsRequest';
import type { BatchReadBlobsResponse as _build_bazel_remote_execution_v2_BatchReadBlobsResponse, BatchReadBlobsResponse__Output as _build_bazel_remote_execution_v2_BatchReadBlobsResponse__Output } from '../../../../../build/bazel/remote/execution/v2/BatchReadBlobsResponse';
import type { BatchUpdateBlobsRequest as _build_bazel_remote_execution_v2_BatchUpdateBlobsRequest, BatchUpdateBlobsRequest__Output as _build_bazel_remote_execution_v2_BatchUpdateBlobsRequest__Output } from '../../../../../build/bazel/remote/execution/v2/BatchUpdateBlobsRequest';
import type { BatchUpdateBlobsResponse as _build_bazel_remote_execution_v2_BatchUpdateBlobsResponse, BatchUpdateBlobsResponse__Output as _build_bazel_remote_execution_v2_BatchUpdateBlobsResponse__Output } from '../../../../../build/bazel/remote/execution/v2/BatchUpdateBlobsResponse';
import type { FindMissingBlobsRequest as _build_bazel_remote_execution_v2_FindMissingBlobsRequest, FindMissingBlobsRequest__Output as _build_bazel_remote_execution_v2_FindMissingBlobsRequest__Output } from '../../../../../build/bazel/remote/execution/v2/FindMissingBlobsRequest';
import type { FindMissingBlobsResponse as _build_bazel_remote_execution_v2_FindMissingBlobsResponse, FindMissingBlobsResponse__Output as _build_bazel_remote_execution_v2_FindMissingBlobsResponse__Output } from '../../../../../build/bazel/remote/execution/v2/FindMissingBlobsResponse';
import type { GetTreeRequest as _build_bazel_remote_execution_v2_GetTreeRequest, GetTreeRequest__Output as _build_bazel_remote_execution_v2_GetTreeRequest__Output } from '../../../../../build/bazel/remote/execution/v2/GetTreeRequest';
import type { GetTreeResponse as _build_bazel_remote_execution_v2_GetTreeResponse, GetTreeResponse__Output as _build_bazel_remote_execution_v2_GetTreeResponse__Output } from '../../../../../build/bazel/remote/execution/v2/GetTreeResponse';

/**
 * The CAS (content-addressable storage) is used to store the inputs to and
 * outputs from the execution service. Each piece of content is addressed by the
 * digest of its binary data.
 * 
 * Most of the binary data stored in the CAS is opaque to the execution engine,
 * and is only used as a communication medium. In order to build an
 * [Action][build.bazel.remote.execution.v2.Action],
 * however, the client will need to also upload the
 * [Command][build.bazel.remote.execution.v2.Command] and input root
 * [Directory][build.bazel.remote.execution.v2.Directory] for the Action.
 * The Command and Directory messages must be marshalled to wire format and then
 * uploaded under the hash as with any other piece of content. In practice, the
 * input root directory is likely to refer to other Directories in its
 * hierarchy, which must also each be uploaded on their own.
 * 
 * For small file uploads the client should group them together and call
 * [BatchUpdateBlobs][build.bazel.remote.execution.v2.ContentAddressableStorage.BatchUpdateBlobs].
 * For large uploads, the client must use the
 * [Write method][google.bytestream.ByteStream.Write] of the ByteStream API. The
 * `resource_name` is `{instance_name}/uploads/{uuid}/blobs/{hash}/{size}`,
 * where `instance_name` is as described in the next paragraph, `uuid` is a
 * version 4 UUID generated by the client, and `hash` and `size` are the
 * [Digest][build.bazel.remote.execution.v2.Digest] of the blob. The
 * `uuid` is used only to avoid collisions when multiple clients try to upload
 * the same file (or the same client tries to upload the file multiple times at
 * once on different threads), so the client MAY reuse the `uuid` for uploading
 * different blobs. The `resource_name` may optionally have a trailing filename
 * (or other metadata) for a client to use if it is storing URLs, as in
 * `{instance}/uploads/{uuid}/blobs/{hash}/{size}/foo/bar/baz.cc`. Anything
 * after the `size` is ignored.
 * 
 * A single server MAY support multiple instances of the execution system, each
 * with their own workers, storage, cache, etc. The exact relationship between
 * instances is up to the server. If the server does, then the `instance_name`
 * is an identifier, possibly containing multiple path segments, used to
 * distinguish between the various instances on the server, in a manner defined
 * by the server. For servers which do not support multiple instances, then the
 * `instance_name` is the empty path and the leading slash is omitted, so that
 * the `resource_name` becomes `uploads/{uuid}/blobs/{hash}/{size}`.
 * To simplify parsing, a path segment cannot equal any of the following
 * keywords: `blobs`, `uploads`, `actions`, `actionResults`, `operations` and
 * `capabilities`.
 * 
 * When attempting an upload, if another client has already completed the upload
 * (which may occur in the middle of a single upload if another client uploads
 * the same blob concurrently), the request will terminate immediately with
 * a response whose `committed_size` is the full size of the uploaded file
 * (regardless of how much data was transmitted by the client). If the client
 * completes the upload but the
 * [Digest][build.bazel.remote.execution.v2.Digest] does not match, an
 * `INVALID_ARGUMENT` error will be returned. In either case, the client should
 * not attempt to retry the upload.
 * 
 * For downloading blobs, the client must use the
 * [Read method][google.bytestream.ByteStream.Read] of the ByteStream API, with
 * a `resource_name` of `"{instance_name}/blobs/{hash}/{size}"`, where
 * `instance_name` is the instance name (see above), and `hash` and `size` are
 * the [Digest][build.bazel.remote.execution.v2.Digest] of the blob.
 * 
 * The lifetime of entries in the CAS is implementation specific, but it SHOULD
 * be long enough to allow for newly-added and recently looked-up entries to be
 * used in subsequent calls (e.g. to
 * [Execute][build.bazel.remote.execution.v2.Execution.Execute]).
 * 
 * As with other services in the Remote Execution API, any call may return an
 * error with a [RetryInfo][google.rpc.RetryInfo] error detail providing
 * information about when the client should retry the request; clients SHOULD
 * respect the information provided.
 */
export interface ContentAddressableStorageClient extends grpc.Client {
  /**
   * Download many blobs at once.
   * 
   * The server may enforce a limit of the combined total size of blobs
   * to be downloaded using this API. This limit may be obtained using the
   * [Capabilities][build.bazel.remote.execution.v2.Capabilities] API.
   * Requests exceeding the limit should either be split into smaller
   * chunks or downloaded using the
   * [ByteStream API][google.bytestream.ByteStream], as appropriate.
   * 
   * This request is equivalent to calling a Bytestream `Read` request
   * on each individual blob, in parallel. The requests may succeed or fail
   * independently.
   * 
   * Errors:
   * 
   * * `INVALID_ARGUMENT`: The client attempted to read more than the
   * server supported limit.
   * 
   * Every error on individual read will be returned in the corresponding digest
   * status.
   */
  BatchReadBlobs(argument: _build_bazel_remote_execution_v2_BatchReadBlobsRequest, metadata: grpc.Metadata, options: grpc.CallOptions, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchReadBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  BatchReadBlobs(argument: _build_bazel_remote_execution_v2_BatchReadBlobsRequest, metadata: grpc.Metadata, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchReadBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  BatchReadBlobs(argument: _build_bazel_remote_execution_v2_BatchReadBlobsRequest, options: grpc.CallOptions, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchReadBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  BatchReadBlobs(argument: _build_bazel_remote_execution_v2_BatchReadBlobsRequest, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchReadBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  /**
   * Download many blobs at once.
   * 
   * The server may enforce a limit of the combined total size of blobs
   * to be downloaded using this API. This limit may be obtained using the
   * [Capabilities][build.bazel.remote.execution.v2.Capabilities] API.
   * Requests exceeding the limit should either be split into smaller
   * chunks or downloaded using the
   * [ByteStream API][google.bytestream.ByteStream], as appropriate.
   * 
   * This request is equivalent to calling a Bytestream `Read` request
   * on each individual blob, in parallel. The requests may succeed or fail
   * independently.
   * 
   * Errors:
   * 
   * * `INVALID_ARGUMENT`: The client attempted to read more than the
   * server supported limit.
   * 
   * Every error on individual read will be returned in the corresponding digest
   * status.
   */
  batchReadBlobs(argument: _build_bazel_remote_execution_v2_BatchReadBlobsRequest, metadata: grpc.Metadata, options: grpc.CallOptions, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchReadBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  batchReadBlobs(argument: _build_bazel_remote_execution_v2_BatchReadBlobsRequest, metadata: grpc.Metadata, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchReadBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  batchReadBlobs(argument: _build_bazel_remote_execution_v2_BatchReadBlobsRequest, options: grpc.CallOptions, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchReadBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  batchReadBlobs(argument: _build_bazel_remote_execution_v2_BatchReadBlobsRequest, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchReadBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  
  /**
   * Upload many blobs at once.
   * 
   * The server may enforce a limit of the combined total size of blobs
   * to be uploaded using this API. This limit may be obtained using the
   * [Capabilities][build.bazel.remote.execution.v2.Capabilities] API.
   * Requests exceeding the limit should either be split into smaller
   * chunks or uploaded using the
   * [ByteStream API][google.bytestream.ByteStream], as appropriate.
   * 
   * This request is equivalent to calling a Bytestream `Write` request
   * on each individual blob, in parallel. The requests may succeed or fail
   * independently.
   * 
   * Errors:
   * 
   * * `INVALID_ARGUMENT`: The client attempted to upload more than the
   * server supported limit.
   * 
   * Individual requests may return the following errors, additionally:
   * 
   * * `RESOURCE_EXHAUSTED`: There is insufficient disk quota to store the blob.
   * * `INVALID_ARGUMENT`: The
   * [Digest][build.bazel.remote.execution.v2.Digest] does not match the
   * provided data.
   */
  BatchUpdateBlobs(argument: _build_bazel_remote_execution_v2_BatchUpdateBlobsRequest, metadata: grpc.Metadata, options: grpc.CallOptions, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchUpdateBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  BatchUpdateBlobs(argument: _build_bazel_remote_execution_v2_BatchUpdateBlobsRequest, metadata: grpc.Metadata, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchUpdateBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  BatchUpdateBlobs(argument: _build_bazel_remote_execution_v2_BatchUpdateBlobsRequest, options: grpc.CallOptions, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchUpdateBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  BatchUpdateBlobs(argument: _build_bazel_remote_execution_v2_BatchUpdateBlobsRequest, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchUpdateBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  /**
   * Upload many blobs at once.
   * 
   * The server may enforce a limit of the combined total size of blobs
   * to be uploaded using this API. This limit may be obtained using the
   * [Capabilities][build.bazel.remote.execution.v2.Capabilities] API.
   * Requests exceeding the limit should either be split into smaller
   * chunks or uploaded using the
   * [ByteStream API][google.bytestream.ByteStream], as appropriate.
   * 
   * This request is equivalent to calling a Bytestream `Write` request
   * on each individual blob, in parallel. The requests may succeed or fail
   * independently.
   * 
   * Errors:
   * 
   * * `INVALID_ARGUMENT`: The client attempted to upload more than the
   * server supported limit.
   * 
   * Individual requests may return the following errors, additionally:
   * 
   * * `RESOURCE_EXHAUSTED`: There is insufficient disk quota to store the blob.
   * * `INVALID_ARGUMENT`: The
   * [Digest][build.bazel.remote.execution.v2.Digest] does not match the
   * provided data.
   */
  batchUpdateBlobs(argument: _build_bazel_remote_execution_v2_BatchUpdateBlobsRequest, metadata: grpc.Metadata, options: grpc.CallOptions, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchUpdateBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  batchUpdateBlobs(argument: _build_bazel_remote_execution_v2_BatchUpdateBlobsRequest, metadata: grpc.Metadata, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchUpdateBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  batchUpdateBlobs(argument: _build_bazel_remote_execution_v2_BatchUpdateBlobsRequest, options: grpc.CallOptions, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchUpdateBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  batchUpdateBlobs(argument: _build_bazel_remote_execution_v2_BatchUpdateBlobsRequest, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_BatchUpdateBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  
  /**
   * Determine if blobs are present in the CAS.
   * 
   * Clients can use this API before uploading blobs to determine which ones are
   * already present in the CAS and do not need to be uploaded again.
   * 
   * There are no method-specific errors.
   */
  FindMissingBlobs(argument: _build_bazel_remote_execution_v2_FindMissingBlobsRequest, metadata: grpc.Metadata, options: grpc.CallOptions, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_FindMissingBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  FindMissingBlobs(argument: _build_bazel_remote_execution_v2_FindMissingBlobsRequest, metadata: grpc.Metadata, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_FindMissingBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  FindMissingBlobs(argument: _build_bazel_remote_execution_v2_FindMissingBlobsRequest, options: grpc.CallOptions, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_FindMissingBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  FindMissingBlobs(argument: _build_bazel_remote_execution_v2_FindMissingBlobsRequest, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_FindMissingBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  /**
   * Determine if blobs are present in the CAS.
   * 
   * Clients can use this API before uploading blobs to determine which ones are
   * already present in the CAS and do not need to be uploaded again.
   * 
   * There are no method-specific errors.
   */
  findMissingBlobs(argument: _build_bazel_remote_execution_v2_FindMissingBlobsRequest, metadata: grpc.Metadata, options: grpc.CallOptions, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_FindMissingBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  findMissingBlobs(argument: _build_bazel_remote_execution_v2_FindMissingBlobsRequest, metadata: grpc.Metadata, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_FindMissingBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  findMissingBlobs(argument: _build_bazel_remote_execution_v2_FindMissingBlobsRequest, options: grpc.CallOptions, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_FindMissingBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  findMissingBlobs(argument: _build_bazel_remote_execution_v2_FindMissingBlobsRequest, callback: (error?: grpc.ServiceError, result?: _build_bazel_remote_execution_v2_FindMissingBlobsResponse__Output) => void): grpc.ClientUnaryCall;
  
  /**
   * Fetch the entire directory tree rooted at a node.
   * 
   * This request must be targeted at a
   * [Directory][build.bazel.remote.execution.v2.Directory] stored in the
   * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage]
   * (CAS). The server will enumerate the `Directory` tree recursively and
   * return every node descended from the root.
   * 
   * The GetTreeRequest.page_token parameter can be used to skip ahead in
   * the stream (e.g. when retrying a partially completed and aborted request),
   * by setting it to a value taken from GetTreeResponse.next_page_token of the
   * last successfully processed GetTreeResponse).
   * 
   * The exact traversal order is unspecified and, unless retrieving subsequent
   * pages from an earlier request, is not guaranteed to be stable across
   * multiple invocations of `GetTree`.
   * 
   * If part of the tree is missing from the CAS, the server will return the
   * portion present and omit the rest.
   * 
   * Errors:
   * 
   * * `NOT_FOUND`: The requested tree root is not present in the CAS.
   */
  GetTree(argument: _build_bazel_remote_execution_v2_GetTreeRequest, metadata: grpc.Metadata, options?: grpc.CallOptions): grpc.ClientReadableStream<_build_bazel_remote_execution_v2_GetTreeResponse__Output>;
  GetTree(argument: _build_bazel_remote_execution_v2_GetTreeRequest, options?: grpc.CallOptions): grpc.ClientReadableStream<_build_bazel_remote_execution_v2_GetTreeResponse__Output>;
  /**
   * Fetch the entire directory tree rooted at a node.
   * 
   * This request must be targeted at a
   * [Directory][build.bazel.remote.execution.v2.Directory] stored in the
   * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage]
   * (CAS). The server will enumerate the `Directory` tree recursively and
   * return every node descended from the root.
   * 
   * The GetTreeRequest.page_token parameter can be used to skip ahead in
   * the stream (e.g. when retrying a partially completed and aborted request),
   * by setting it to a value taken from GetTreeResponse.next_page_token of the
   * last successfully processed GetTreeResponse).
   * 
   * The exact traversal order is unspecified and, unless retrieving subsequent
   * pages from an earlier request, is not guaranteed to be stable across
   * multiple invocations of `GetTree`.
   * 
   * If part of the tree is missing from the CAS, the server will return the
   * portion present and omit the rest.
   * 
   * Errors:
   * 
   * * `NOT_FOUND`: The requested tree root is not present in the CAS.
   */
  getTree(argument: _build_bazel_remote_execution_v2_GetTreeRequest, metadata: grpc.Metadata, options?: grpc.CallOptions): grpc.ClientReadableStream<_build_bazel_remote_execution_v2_GetTreeResponse__Output>;
  getTree(argument: _build_bazel_remote_execution_v2_GetTreeRequest, options?: grpc.CallOptions): grpc.ClientReadableStream<_build_bazel_remote_execution_v2_GetTreeResponse__Output>;
  
}

/**
 * The CAS (content-addressable storage) is used to store the inputs to and
 * outputs from the execution service. Each piece of content is addressed by the
 * digest of its binary data.
 * 
 * Most of the binary data stored in the CAS is opaque to the execution engine,
 * and is only used as a communication medium. In order to build an
 * [Action][build.bazel.remote.execution.v2.Action],
 * however, the client will need to also upload the
 * [Command][build.bazel.remote.execution.v2.Command] and input root
 * [Directory][build.bazel.remote.execution.v2.Directory] for the Action.
 * The Command and Directory messages must be marshalled to wire format and then
 * uploaded under the hash as with any other piece of content. In practice, the
 * input root directory is likely to refer to other Directories in its
 * hierarchy, which must also each be uploaded on their own.
 * 
 * For small file uploads the client should group them together and call
 * [BatchUpdateBlobs][build.bazel.remote.execution.v2.ContentAddressableStorage.BatchUpdateBlobs].
 * For large uploads, the client must use the
 * [Write method][google.bytestream.ByteStream.Write] of the ByteStream API. The
 * `resource_name` is `{instance_name}/uploads/{uuid}/blobs/{hash}/{size}`,
 * where `instance_name` is as described in the next paragraph, `uuid` is a
 * version 4 UUID generated by the client, and `hash` and `size` are the
 * [Digest][build.bazel.remote.execution.v2.Digest] of the blob. The
 * `uuid` is used only to avoid collisions when multiple clients try to upload
 * the same file (or the same client tries to upload the file multiple times at
 * once on different threads), so the client MAY reuse the `uuid` for uploading
 * different blobs. The `resource_name` may optionally have a trailing filename
 * (or other metadata) for a client to use if it is storing URLs, as in
 * `{instance}/uploads/{uuid}/blobs/{hash}/{size}/foo/bar/baz.cc`. Anything
 * after the `size` is ignored.
 * 
 * A single server MAY support multiple instances of the execution system, each
 * with their own workers, storage, cache, etc. The exact relationship between
 * instances is up to the server. If the server does, then the `instance_name`
 * is an identifier, possibly containing multiple path segments, used to
 * distinguish between the various instances on the server, in a manner defined
 * by the server. For servers which do not support multiple instances, then the
 * `instance_name` is the empty path and the leading slash is omitted, so that
 * the `resource_name` becomes `uploads/{uuid}/blobs/{hash}/{size}`.
 * To simplify parsing, a path segment cannot equal any of the following
 * keywords: `blobs`, `uploads`, `actions`, `actionResults`, `operations` and
 * `capabilities`.
 * 
 * When attempting an upload, if another client has already completed the upload
 * (which may occur in the middle of a single upload if another client uploads
 * the same blob concurrently), the request will terminate immediately with
 * a response whose `committed_size` is the full size of the uploaded file
 * (regardless of how much data was transmitted by the client). If the client
 * completes the upload but the
 * [Digest][build.bazel.remote.execution.v2.Digest] does not match, an
 * `INVALID_ARGUMENT` error will be returned. In either case, the client should
 * not attempt to retry the upload.
 * 
 * For downloading blobs, the client must use the
 * [Read method][google.bytestream.ByteStream.Read] of the ByteStream API, with
 * a `resource_name` of `"{instance_name}/blobs/{hash}/{size}"`, where
 * `instance_name` is the instance name (see above), and `hash` and `size` are
 * the [Digest][build.bazel.remote.execution.v2.Digest] of the blob.
 * 
 * The lifetime of entries in the CAS is implementation specific, but it SHOULD
 * be long enough to allow for newly-added and recently looked-up entries to be
 * used in subsequent calls (e.g. to
 * [Execute][build.bazel.remote.execution.v2.Execution.Execute]).
 * 
 * As with other services in the Remote Execution API, any call may return an
 * error with a [RetryInfo][google.rpc.RetryInfo] error detail providing
 * information about when the client should retry the request; clients SHOULD
 * respect the information provided.
 */
export interface ContentAddressableStorageHandlers extends grpc.UntypedServiceImplementation {
  /**
   * Download many blobs at once.
   * 
   * The server may enforce a limit of the combined total size of blobs
   * to be downloaded using this API. This limit may be obtained using the
   * [Capabilities][build.bazel.remote.execution.v2.Capabilities] API.
   * Requests exceeding the limit should either be split into smaller
   * chunks or downloaded using the
   * [ByteStream API][google.bytestream.ByteStream], as appropriate.
   * 
   * This request is equivalent to calling a Bytestream `Read` request
   * on each individual blob, in parallel. The requests may succeed or fail
   * independently.
   * 
   * Errors:
   * 
   * * `INVALID_ARGUMENT`: The client attempted to read more than the
   * server supported limit.
   * 
   * Every error on individual read will be returned in the corresponding digest
   * status.
   */
  BatchReadBlobs: grpc.handleUnaryCall<_build_bazel_remote_execution_v2_BatchReadBlobsRequest__Output, _build_bazel_remote_execution_v2_BatchReadBlobsResponse>;
  
  /**
   * Upload many blobs at once.
   * 
   * The server may enforce a limit of the combined total size of blobs
   * to be uploaded using this API. This limit may be obtained using the
   * [Capabilities][build.bazel.remote.execution.v2.Capabilities] API.
   * Requests exceeding the limit should either be split into smaller
   * chunks or uploaded using the
   * [ByteStream API][google.bytestream.ByteStream], as appropriate.
   * 
   * This request is equivalent to calling a Bytestream `Write` request
   * on each individual blob, in parallel. The requests may succeed or fail
   * independently.
   * 
   * Errors:
   * 
   * * `INVALID_ARGUMENT`: The client attempted to upload more than the
   * server supported limit.
   * 
   * Individual requests may return the following errors, additionally:
   * 
   * * `RESOURCE_EXHAUSTED`: There is insufficient disk quota to store the blob.
   * * `INVALID_ARGUMENT`: The
   * [Digest][build.bazel.remote.execution.v2.Digest] does not match the
   * provided data.
   */
  BatchUpdateBlobs: grpc.handleUnaryCall<_build_bazel_remote_execution_v2_BatchUpdateBlobsRequest__Output, _build_bazel_remote_execution_v2_BatchUpdateBlobsResponse>;
  
  /**
   * Determine if blobs are present in the CAS.
   * 
   * Clients can use this API before uploading blobs to determine which ones are
   * already present in the CAS and do not need to be uploaded again.
   * 
   * There are no method-specific errors.
   */
  FindMissingBlobs: grpc.handleUnaryCall<_build_bazel_remote_execution_v2_FindMissingBlobsRequest__Output, _build_bazel_remote_execution_v2_FindMissingBlobsResponse>;
  
  /**
   * Fetch the entire directory tree rooted at a node.
   * 
   * This request must be targeted at a
   * [Directory][build.bazel.remote.execution.v2.Directory] stored in the
   * [ContentAddressableStorage][build.bazel.remote.execution.v2.ContentAddressableStorage]
   * (CAS). The server will enumerate the `Directory` tree recursively and
   * return every node descended from the root.
   * 
   * The GetTreeRequest.page_token parameter can be used to skip ahead in
   * the stream (e.g. when retrying a partially completed and aborted request),
   * by setting it to a value taken from GetTreeResponse.next_page_token of the
   * last successfully processed GetTreeResponse).
   * 
   * The exact traversal order is unspecified and, unless retrieving subsequent
   * pages from an earlier request, is not guaranteed to be stable across
   * multiple invocations of `GetTree`.
   * 
   * If part of the tree is missing from the CAS, the server will return the
   * portion present and omit the rest.
   * 
   * Errors:
   * 
   * * `NOT_FOUND`: The requested tree root is not present in the CAS.
   */
  GetTree: grpc.handleServerStreamingCall<_build_bazel_remote_execution_v2_GetTreeRequest__Output, _build_bazel_remote_execution_v2_GetTreeResponse>;
  
}

export interface ContentAddressableStorageDefinition extends grpc.ServiceDefinition {
  BatchReadBlobs: MethodDefinition<_build_bazel_remote_execution_v2_BatchReadBlobsRequest, _build_bazel_remote_execution_v2_BatchReadBlobsResponse, _build_bazel_remote_execution_v2_BatchReadBlobsRequest__Output, _build_bazel_remote_execution_v2_BatchReadBlobsResponse__Output>
  BatchUpdateBlobs: MethodDefinition<_build_bazel_remote_execution_v2_BatchUpdateBlobsRequest, _build_bazel_remote_execution_v2_BatchUpdateBlobsResponse, _build_bazel_remote_execution_v2_BatchUpdateBlobsRequest__Output, _build_bazel_remote_execution_v2_BatchUpdateBlobsResponse__Output>
  FindMissingBlobs: MethodDefinition<_build_bazel_remote_execution_v2_FindMissingBlobsRequest, _build_bazel_remote_execution_v2_FindMissingBlobsResponse, _build_bazel_remote_execution_v2_FindMissingBlobsRequest__Output, _build_bazel_remote_execution_v2_FindMissingBlobsResponse__Output>
  GetTree: MethodDefinition<_build_bazel_remote_execution_v2_GetTreeRequest, _build_bazel_remote_execution_v2_GetTreeResponse, _build_bazel_remote_execution_v2_GetTreeRequest__Output, _build_bazel_remote_execution_v2_GetTreeResponse__Output>
}
